include/columnar_reader.h --- 1/6 --- C++
4    #include <atomic>
5    #include <intermediate.h>
6    #include <plan.h>
7    #include <vector>
15    /**
16     *
17     *  page index for efficient row lookup in ColumnarTable pages
18     *  pre-computed metadata to accelerate random access during materialization
19     *
20     *  cumulative_rows stores running total of rows up to each page
21     *  enables binary search to locate which page contains a given row_id
22     *
23     *  page_prefix_sums accelerates sparse page bitmap navigation
24     *  sparse pages have bitmap marking valid non-NULL rows
25     *  to find data index for row N requires counting valid bits before it
26     *  without optimization this is O(N) bit-by-bit counting
27     *  page_prefix_sums divides bitmap into 64-bit chunks with pre-computed
28     *  popcounts reduces to O(1) array lookup plus one popcount on final 64-bit chunk
29     *
30     **/
35        /**
36         *
37         *  builds page index for entire column
38         *  processes all pages to construct cumulative_rows and page_prefix_sums
39         *  called once per column during prepare_build or prepare_probe
40         *
41         **/
50                if (num_rows == 0xfffe) {
51                } else if (num_rows == 0xffff) {
   8 #include <cstring> // Added for memcpy
   10 /* likely/unlikely macros for branch prediction optimization */
   11 #define SPC_LIKELY(x) __builtin_expect(!!(x), 1)
   12 #define SPC_UNLIKELY(x) __builtin_expect(!!(x), 0)
   20 /**
   21  *
   22  * page index for efficient row lookup in ColumnarTable pages
   23  * pre-computed metadata to accelerate random access during materialization
   24  *
   25  **/
   30     /**
   31      *
   32      * builds page index for entire column
   33      * processes all pages to construct cumulative_rows and page_prefix_sums
   34      *
   35      **/
   39         cumulative_rows.reserve(column.pages.size());
   45             if (SPC_UNLIKELY(num_rows == 0xfffe)) {
   46                 /* long string continuation - contributes 0 logical rows */
   47             } else if (SPC_UNLIKELY(num_rows == 0xffff)) {
   48                 /* long string start - contributes 1 row */
   49                 total += 1;
   50             } else {
   51                 total += num_rows;
   52             }

include/columnar_reader.h --- 2/6 --- C++
57    
58                auto num_values = *reinterpret_cast<const uint16_t *>(page + 2);
59                std::vector<uint32_t> prefix_sums;
63                    auto *bitmap = reinterpret_cast<const uint64_t *>(
64                        page + PAGE_SIZE - (num_rows + 7) / 8);
71                        sum += __builtin_popcountll(bitmap[i]);
78        /**
79         *
80         *  binary search to find which page contains row_id
81         *  uses cumulative_rows for O(log P) lookup where P is page count
82         *  returns page index that contains the requested row
83         *
84         **/
88            size_t page_idx = (it - cumulative_rows.begin());
89            return page_idx;
92        /**
93         *
94         *  returns starting row number for given page
95         *  used to compute local row offset within page
96         *
97         **/
103    /**
104     *
105     *  efficient reader for ColumnarTable inputs during materialization
106     *  provides two-level optimization for random row access
107     *
108     *  pre-computed for all pages during prepare_build or prepare_probe
109     *  enables fast page lookup and O(1) sparse page data index calculation
110     *
111     *  lazily populated during reads to exploit spatial locality
112     *  when consecutive reads target same page avoids binary search entirely
113     *  hash joins produce clustered matches from same hash bucket
114     *  nested loops iterate sequentially through pages
115     *
116     *  maintains separate indices and caches for build and probe sides
117     *
118     **/
123        /**
124         *
125         *  builds PageIndex for all build side and probe side columns
126         *  pre-computes cumulative_rows and page_prefix_sums
127         *  resets last accessed page cache to invalid state
128         *
129         **/
   58             /* sparse page check */
   62                 /* FIX: Use uint8_t* to calculate offset safely */
   63                 size_t bitmap_size = (num_rows + 7) / 8;
   64                 auto *bitmap_bytes = reinterpret_cast<const uint8_t *>(
   65                     page + PAGE_SIZE - bitmap_size);
   74                     /* FIX: Safe read to avoid reading past PAGE_SIZE/bitmap end */
   75                     uint64_t word = 0;
   76                     size_t offset = i * 8;
   77                     size_t remaining = bitmap_size > offset ? bitmap_size - offset : 0;
   78                     size_t bytes_to_read = std::min(remaining, size_t(8));
   80                     if (bytes_to_read > 0) {
   81                         std::memcpy(&word, bitmap_bytes + offset, bytes_to_read);
   82                     }
   84                     sum += __builtin_popcountll(word);
   91     /**
   92      *
   93      * binary search to find which page contains row_id
   94      * uses cumulative_rows for O(log P) lookup
   95      *
   96      **/
   100         return static_cast<size_t>(it - cumulative_rows.begin());
   103     /**
   104      *
   105      * returns starting row number for given page
   106      *
   107      **/
   113 /**
   114  *
   115  * efficient reader for ColumnarTable inputs
   116  * provides two-level optimization for random row access
   117  * lazily populated caches exploit spatial locality
   118  *
   119  **/
   124     inline void prepare_build(const std::vector<const Column *> &columns) {
   125         build_page_indices.clear();
   126         build_page_indices.reserve(columns.size());
   127         for (const auto *column : columns) {

include/columnar_reader.h --- 3/6 --- C++
134                PageIndex page_idx;
135                page_idx.build(*column);
136                build_page_indices.push_back(std::move(page_idx));
137            }
138            /* increment global version to invalidate all thread_local caches */
139            global_build_version.fetch_add(1, std::memory_order_release);
   132         global_build_version.fetch_add(1, std::memory_order_relaxed);
   133     }
   134 
   135     inline void prepare_probe(const std::vector<const Column *> &columns) {
   136         probe_page_indices.clear();

include/columnar_reader.h --- 4/6 --- C++
146                PageIndex page_idx;
147                page_idx.build(*column);
148                probe_page_indices.push_back(std::move(page_idx));
149            }
150            /* increment global version to invalidate all thread_local caches */
151            global_probe_version.fetch_add(1, std::memory_order_release);
154        inline mema::value_t read_value_build(const Column &column, size_t col_idx,
156            /* thread-local cache for parallel materialization */
157            thread_local size_t tl_build_cached_col = ~0u;
158            thread_local size_t tl_build_cached_page = ~0u;
159            thread_local uint32_t tl_build_cached_start = 0;
160            thread_local uint32_t tl_build_cached_end = 0;
161            thread_local uint64_t tl_build_version = 0;
163            uint64_t current_version = global_build_version.load(std::memory_order_acquire);
165            if (tl_build_version == current_version &&
166                col_idx == tl_build_cached_col && row_id >= tl_build_cached_start &&
167                row_id < tl_build_cached_end) {
168                return read_from_page(column, build_page_indices[col_idx],
169                                      tl_build_cached_page,
170                                      row_id - tl_build_cached_start, data_type);
178            tl_build_version = current_version;
179            tl_build_cached_col = col_idx;
180            tl_build_cached_page = page_num;
181            tl_build_cached_start = page_start;
182            tl_build_cached_end = page_end;
188        inline mema::value_t read_value_probe(const Column &column, size_t col_idx,
189                                              uint32_t row_id, DataType data_type) const {
190            /* thread-local cache for parallel materialization */
191            thread_local size_t tl_probe_cached_col = ~0u;
192            thread_local size_t tl_probe_cached_page = ~0u;
193            thread_local uint32_t tl_probe_cached_start = 0;
194            thread_local uint32_t tl_probe_cached_end = 0;
195            thread_local uint64_t tl_probe_version = 0;
197            uint64_t current_version = global_probe_version.load(std::memory_order_acquire);
199            if (tl_probe_version == current_version &&
200                col_idx == tl_probe_cached_col && row_id >= tl_probe_cached_start &&
201                row_id < tl_probe_cached_end) {
202                return read_from_page(column, probe_page_indices[col_idx],
203                                      tl_probe_cached_page,
204                                      row_id - tl_probe_cached_start, data_type);
207            const PageIndex &page_index = probe_page_indices[col_idx];
208            size_t page_num = page_index.find_page(row_id);
209            uint32_t page_start = page_index.page_start_row(page_num);
210            uint32_t page_end = page_index.cumulative_rows[page_num];
212            tl_probe_version = current_version;
213            tl_probe_cached_col = col_idx;
214            tl_probe_cached_page = page_num;
215            tl_probe_cached_start = page_start;
216            tl_probe_cached_end = page_end;
218            return read_from_page(column, page_index, page_num, row_id - page_start,
219                                  data_type);
220        }
   143         global_probe_version.fetch_add(1, std::memory_order_relaxed);
   146     /* internal implementation template to deduplicate build/probe logic */
   147     template <bool IsBuild>
   148     inline mema::value_t read_value_internal(const Column &column, size_t col_idx,
   150         /* thread-local cache */
   151         thread_local size_t tl_cached_col = ~0u;
   152         thread_local size_t tl_cached_page = ~0u;
   153         thread_local uint32_t tl_cached_start = 0;
   154         thread_local uint32_t tl_cached_end = 0;
   155         thread_local uint64_t tl_version = 0;
   157         /* relaxed load is sufficient for cache validation in this context */
   158         uint64_t current_version;
   159         if constexpr (IsBuild) {
   160             current_version = global_build_version.load(std::memory_order_relaxed);
   161         } else {
   162             current_version = global_probe_version.load(std::memory_order_relaxed);
   163         }
   165         /* fast path: cache hit */
   166         if (SPC_LIKELY(tl_version == current_version &&
   167                        col_idx == tl_cached_col && 
   168                        row_id >= tl_cached_start &&
   169                        row_id < tl_cached_end)) {
   171             const auto& indices = IsBuild ? build_page_indices : probe_page_indices;
   172             return read_from_page(column, indices[col_idx],
   173                                   tl_cached_page,
   174                                   row_id - tl_cached_start, data_type);
   177         /* slow path: cache miss / version mismatch */
   178         const PageIndex &page_index = IsBuild ? build_page_indices[col_idx] 
   179                                               : probe_page_indices[col_idx];
   185         tl_version = current_version;
   186         tl_cached_col = col_idx;
   187         tl_cached_page = page_num;
   188         tl_cached_start = page_start;
   189         tl_cached_end = page_end;
   195     inline mema::value_t read_value_build(const Column &column, size_t col_idx,
   196                                           uint32_t row_id, DataType data_type) const {
   197         return read_value_internal<true>(column, col_idx, row_id, data_type);
   198     }
   200     inline mema::value_t read_value_probe(const Column &column, size_t col_idx,
   201                                           uint32_t row_id, DataType data_type) const {
   202         return read_value_internal<false>(column, col_idx, row_id, data_type);
   205     inline const PageIndex &get_build_page_index(size_t col_idx) const {
   206         return build_page_indices[col_idx];
   207     }

include/columnar_reader.h --- 5/6 --- C++
235            auto *page = column.pages[page_num]->data;
236            auto num_rows = *reinterpret_cast<const uint16_t *>(page);
237            auto num_values = *reinterpret_cast<const uint16_t *>(page + 2);
239            if (num_rows == 0xffff) {
241                    page_num, mema::value_t::LONG_STRING_OFFSET);
246            if (num_rows == num_values) {
247                if (data_type == DataType::INT32) {
248                    return mema::value_t{data_begin[local_row]};
249                } else {
250                    return mema::value_t::encode_string(page_num, local_row);
251                }
252            } else {
254                    page + PAGE_SIZE - (num_rows + 7) / 8);
255                bool is_valid = bitmap[local_row / 8] & (1u << (local_row % 8));
   222         /* check for long string continuation */
   223         if (SPC_UNLIKELY(num_rows == 0xffff)) {
   225                 static_cast<int32_t>(page_num), mema::value_t::LONG_STRING_OFFSET);
   230         /* dense page optimization (no nulls) */
   231         if (SPC_LIKELY(num_rows == num_values)) {
   232             if (data_type == DataType::INT32) {
   233                 return mema::value_t{data_begin[local_row]};
   234             } else {
   235                 return mema::value_t::encode_string(static_cast<int32_t>(page_num), 
   236                                                     static_cast<int32_t>(local_row));
   237             }
   238         } 
   240         /* sparse page handling */
   241         /* FIX: Use uint8_t* for correct pointer arithmetic and sizing */
   242         size_t bitmap_size = (num_rows + 7) / 8;
   244             page + PAGE_SIZE - bitmap_size);
   246         /* optimized bit check: replace div/mod with shifts */
   247         bool is_valid = bitmap[local_row >> 3] & (1u << (local_row & 7));
   249         if (!is_valid) {
   250             return mema::value_t{mema::value_t::NULL_VALUE};
   251         }

include/columnar_reader.h --- 6/6 --- C++
263                size_t bit_offset = local_row & 0x3F;
264    
265                uint16_t data_idx = prefix_sums[chunk_idx];
267                auto *bitmap64 = reinterpret_cast<const uint64_t *>(bitmap);
269                data_idx += __builtin_popcountll(bitmap64[chunk_idx] & mask);
274                    return mema::value_t::encode_string(page_num, data_idx);
276            }
   259         /* FIX: Safe read to avoid reading past PAGE_SIZE/bitmap end */
   260         uint64_t word = 0;
   261         size_t offset = chunk_idx * 8;
   262         size_t remaining = bitmap_size > offset ? bitmap_size - offset : 0;
   263         size_t bytes_to_read = std::min(remaining, size_t(8));
   265         if (bytes_to_read > 0) {
   266             std::memcpy(&word, bitmap + offset, bytes_to_read);
   267         }
   269         /* create mask for bits preceding current position */
   271         data_idx += __builtin_popcountll(word & mask);
   276             return mema::value_t::encode_string(static_cast<int32_t>(page_num), 
   277                                                 static_cast<int32_t>(data_idx));
   279     }
   280 
   281     std::vector<PageIndex> build_page_indices;
   282     std::vector<PageIndex> probe_page_indices;

include/construct_intermediate.h --- 1/3 --- C++
181     **/
182    inline std::shared_ptr<BatchAllocator> batch_allocate_for_results(
183        ExecuteResult& results, size_t total_matches) {
184    
185        size_t total_pages = 0;
187            total_pages += (total_matches + mema::CAP_PER_PAGE - 1) / mema::CAP_PER_PAGE;
191        allocator->allocate(total_pages);
   185     size_t total_chunks = 0;
   187         /* Calculate how many logical chunks (mema::column_t::Page) are needed */
   188         total_chunks += (total_matches + mema::CAP_PER_PAGE - 1) / mema::CAP_PER_PAGE;
   191     /* 
   192      * FIX: Calculate total system pages required.
   193      * Each mema::column_t::Page holds CAP_PER_PAGE * sizeof(value_t) bytes.
   194      * CAP_PER_PAGE = 2048, sizeof(value_t) = 4, so Chunk Size = 8192 bytes.
   195      * System PAGE_SIZE is typically 4096 bytes.
   196      * We must allocate enough system pages to cover the chunk size.
   197      */
   198     size_t bytes_per_chunk = mema::CAP_PER_PAGE * sizeof(mema::value_t);
   199     size_t total_bytes = total_chunks * bytes_per_chunk;
   200     size_t system_pages = (total_bytes + PAGE_SIZE - 1) / PAGE_SIZE;
   203     allocator->allocate(system_pages);
   204     void* block = allocator->get_block();
   205     size_t offset = 0;
   206 
   207     for (auto& col : results) {

include/construct_intermediate.h --- 2/3 --- C++
274    
275    struct JoinInput;
276    class ColumnarReader;
277    
278    /**
279     *
280     *  constructs intermediate results from column_t intermediate format
281     *  both build and probe are intermediate results
282     *  parallel implementation using multiple threads
283     *
284     **/
285    inline void construct_intermediate_from_intermediate(
286        const MatchCollector &collector, const ExecuteResult &build,
287        const ExecuteResult &probe,
288        const std::vector<std::tuple<size_t, DataType>> &output_attrs,
289        ExecuteResult &results) {
290        const size_t total_matches = collector.size();
291        if (total_matches == 0)
292            return;
294        constexpr size_t PARALLEL_THRESHOLD = 5000;
295        if (total_matches < PARALLEL_THRESHOLD) {
296            const uint64_t *matches_ptr = collector.matches.data();
297            const size_t build_size = build.size();
298            for (size_t out_idx = 0; out_idx < output_attrs.size(); ++out_idx) {
299                auto [col_idx, _] = output_attrs[out_idx];
300                const bool from_build = col_idx < build_size;
301                const mema::column_t *column = from_build ? &build[col_idx] : &probe[col_idx - build_size];
302                const uint32_t shift = from_build ? 0 : 32;
303                results[out_idx].reserve(total_matches);
304                for (size_t match_idx = 0; match_idx < total_matches; ++match_idx) {
305                    uint32_t row_id = (matches_ptr[match_idx] >> shift);
306                    const mema::value_t &val = (*column)[row_id];
307                    results[out_idx].append(val);
308                }
309            }
310            return;
311        }
313        const uint64_t *matches_ptr = collector.matches.data();
314        const size_t build_size = build.size();
315        const size_t chunk_size = compute_chunk_size(output_attrs.size(), total_matches);
316        const auto tasks = compute_tasks(output_attrs.size(), total_matches, chunk_size);
317        const auto sources = prepare_sources(output_attrs, build, probe, build_size);
319        batch_allocate_for_results(results, total_matches);
321        worker_pool.execute([&](size_t t, size_t pool_size) {
322            for (size_t task_idx = t; task_idx < tasks.size(); task_idx += pool_size) {
323                const auto& task = tasks[task_idx];
324                const auto& src = sources[task.col_idx];
325                auto& dest = results[task.col_idx];
326                for (size_t i = task.chunk_start; i < task.chunk_end; ++i) {
327                    const uint32_t row_id = static_cast<uint32_t>(matches_ptr[i] >> src.shift);
328                    dest.write_at(i, (*src.column)[row_id]);
329                }
330            }
331        });
332    }
334    /**
335     *
336     *  constructs intermediate results directly from ColumnarTable columnar inputs
337     *  both build and probe are reading from original page format
338     *  parallel implementation using multiple threads
339     *
340     **/
341    inline void construct_intermediate_from_columnar(
   290 /**
   291  *
   292  * constructs intermediate results (vectors of values) from join matches
   293  * parallelized construction of non-columnar intermediate results
   294  *
   295  **/
   296 inline void construct_intermediate(
   297     const MatchCollector &collector, const JoinInput &build_input,
   298     const JoinInput &probe_input,
   299     const std::vector<std::tuple<size_t, DataType>> &remapped_attrs,
   300     const PlanNode &build_node, const PlanNode &probe_node, size_t build_size,

include/construct_intermediate.h --- 3/3 --- C++
348        const size_t total_matches = collector.size();
349        if (total_matches == 0)
350            return;
353        const size_t chunk_size = compute_chunk_size(remapped_attrs.size(), total_matches);
354        const auto tasks = compute_tasks(remapped_attrs.size(), total_matches, chunk_size);
355        const auto sources = prepare_columnar_sources(remapped_attrs, build_input, probe_input,
356                                                       build_node, probe_node, build_size);
358        batch_allocate_for_results(results, total_matches);
360        worker_pool.execute([&](size_t t, size_t pool_size) {
361            for (size_t task_idx = t; task_idx < tasks.size(); task_idx += pool_size) {
362                const auto& task = tasks[task_idx];
363                const auto& src = sources[task.col_idx];
364                auto& dest = results[task.col_idx];
365                for (size_t i = task.chunk_start; i < task.chunk_end; ++i) {
366                    const uint32_t row_id = static_cast<uint32_t>(matches_ptr[i] >> src.shift);
367                    mema::value_t value = src.from_build
368                                              ? columnar_reader.read_value_build(
369                                                    *src.column, src.remapped_col_idx, row_id,
370                                                    src.column->type)
371                                              : columnar_reader.read_value_probe(
372                                                    *src.column, src.remapped_col_idx, row_id,
373                                                    src.column->type);
374                    dest.write_at(i, value);
380    /**
381     *
382     *  constructs intermediate results from mixed input types
383     *  one side is ColumnarTable columnar other is column_t intermediate
384     *  parallel implementation using multiple threads
385     *
386     **/
387    inline void construct_intermediate_mixed(
388        const MatchCollector &collector, const JoinInput &build_input,
389        const JoinInput &probe_input,
390        const std::vector<std::tuple<size_t, DataType>> &remapped_attrs,
391        const PlanNode &build_node, const PlanNode &probe_node, size_t build_size,
392        ColumnarReader &columnar_reader, ExecuteResult &results) {
394        const size_t total_matches = collector.size();
395        if (total_matches == 0)
396            return;
398        const uint64_t *matches_ptr = collector.matches.data();
399        const size_t chunk_size = compute_chunk_size(remapped_attrs.size(), total_matches);
400        const auto tasks = compute_tasks(remapped_attrs.size(), total_matches, chunk_size);
401        const auto sources = prepare_mixed_sources(remapped_attrs, build_input, probe_input,
402                                                    build_node, probe_node, build_size);
404        batch_allocate_for_results(results, total_matches);
406        worker_pool.execute([&](size_t t, size_t pool_size) {
407            for (size_t task_idx = t; task_idx < tasks.size(); task_idx += pool_size) {
408                const auto& task = tasks[task_idx];
409                const auto& src = sources[task.col_idx];
410                auto& dest = results[task.col_idx];
411                for (size_t i = task.chunk_start; i < task.chunk_end; ++i) {
412                    const uint32_t row_id = static_cast<uint32_t>(matches_ptr[i] >> src.shift);
414                    mema::value_t value;
415                    if (src.is_columnar) {
416                        value = src.from_build
417                                    ? columnar_reader.read_value_build(
418                                          *src.columnar_col, src.remapped_col_idx,
419                                          row_id, src.columnar_col->type)
420                                    : columnar_reader.read_value_probe(
421                                          *src.columnar_col, src.remapped_col_idx,
422                                          row_id, src.columnar_col->type);
423                    } else {
424                        value = (*src.intermediate_col)[row_id];
425                    }
426                    dest.write_at(i, value);
427                }
428            }
429        });
430    }
   306     /* 
   307      * Pre-allocate memory using the batch allocator.
   308      * This replaces the individual resize() calls which would do individual allocs.
   309      * Note: This function now correctly handles 8KB chunks vs 4KB system pages.
   310      */
   311     auto allocator = batch_allocate_for_results(results, total_matches);
   314     constexpr int num_threads = SPC__CORE_COUNT;
   316     worker_pool.execute([&](size_t t, size_t num_threads) {
   317         size_t start = t * total_matches / num_threads;
   318         size_t end = (t + 1) * total_matches / num_threads;
   319         if (start >= end) return;
   321         /* iterate over each requested output column */
   322         for (size_t out_idx = 0; out_idx < remapped_attrs.size(); ++out_idx) {
   323             auto [col_idx, data_type] = remapped_attrs[out_idx];
   324             bool from_build = col_idx < build_size;
   325             size_t remapped_col_idx = from_build ? col_idx : col_idx - build_size;
   327             /* reference to destination column (paged vector) */
   328             auto& dest_col = results[out_idx];
   330             /* resolve data source */
   331             const Column *columnar_src_col = nullptr;
   332             const mema::column_t *intermediate_src_col = nullptr;
   334             if (from_build) {
   335                 if (build_input.is_columnar()) {
   336                     auto *table = std::get<const ColumnarTable *>(build_input.data);
   337                     auto [actual_idx, _] = build_node.output_attrs[remapped_col_idx];
   338                     columnar_src_col = &table->columns[actual_idx];
   339                 } else {
   340                     const auto &res = std::get<ExecuteResult>(build_input.data);
   341                     intermediate_src_col = &res[remapped_col_idx];
   342                 }
   343             } else {
   344                 if (probe_input.is_columnar()) {
   345                     auto *table = std::get<const ColumnarTable *>(probe_input.data);
   346                     auto [actual_idx, _] = probe_node.output_attrs[remapped_col_idx];
   347                     columnar_src_col = &table->columns[actual_idx];
   348                 } else {
   349                     const auto &res = std::get<ExecuteResult>(probe_input.data);
   350                     intermediate_src_col = &res[remapped_col_idx];
   351                 }
   352             }
   354             /* inner tight loop for filling this column chunk */
   355             if (columnar_src_col) {
   356                 const auto& col = *columnar_src_col;
   357                 if (from_build) {
   358                     for (size_t i = start; i < end; ++i) {
   359                         uint32_t rid = static_cast<uint32_t>(matches_ptr[i]);
   360                         /* use write_at for thread-safe access to paged storage */
   361                         dest_col.write_at(i, columnar_reader.read_value_build(
   362                             col, remapped_col_idx, rid, data_type));
   363                     }
   364                 } else {
   365                     for (size_t i = start; i < end; ++i) {
   366                         uint32_t rid = static_cast<uint32_t>(matches_ptr[i] >> 32);
   367                         dest_col.write_at(i, columnar_reader.read_value_probe(
   368                             col, remapped_col_idx, rid, data_type));
   369                     }
   371             } else {
   372                 /* fast copy from intermediate vector */
   373                 const auto& src_vec = *intermediate_src_col;
   374                 if (from_build) {
   375                     for (size_t i = start; i < end; ++i) {
   376                         uint32_t rid = static_cast<uint32_t>(matches_ptr[i]);
   377                         dest_col.write_at(i, src_vec[rid]);
   378                     }
   379                 } else {
   380                     for (size_t i = start; i < end; ++i) {
   381                         uint32_t rid = static_cast<uint32_t>(matches_ptr[i] >> 32);
   382                         dest_col.write_at(i, src_vec[rid]);
   383                     }
   384                 }
   385             }
   389 } // namespace Contest

include/intermediate.h --- 1/3 --- C++
8    #include <memory>
9    
10    namespace mema {
11    
12    /**
13     *
14     *  value_t struct holding necessary metadata.
15     *  for INT32 we store the integer value directly
16     *  For VARCHAR we pack 19 bits for page_idx and
17     *  13 bits for offset_idx this fits all pages
18     *  and allows the neccesary offset index
19     *
20     **/
24        /* encodes string metadata by packing bits */
29        /**
30         *
31         *  decodes first 19 bits for page_idx casts
32         *  to prevent sign extension when shifting
33         *  signed integer and decodes 13 bits for
34         *  offset idx
35         *
36         **/
44        /* when we encounter long string all offset bits are set */
47        /* sentinel value to represent NULL for both INT32 and VARCHAR */
53    constexpr size_t CAP_PER_PAGE = PAGE_SIZE / sizeof(value_t);
55    /**
56     *
57     *  CAP_PER_PAGE = 2048 to achieve 100% memory utilization per page
58     *  a simple vector of pages with value_t append function that  writes value
59     *  sequentially to the end,and also checks if new page is needed.
60     *  and also an operator to read the value from the idx
61     *
62     **/
65        /* added page alignment */
66        struct alignas(PAGE_SIZE) Page {
   12 /**
   13  *
   14  * value_t struct holding necessary metadata.
   15  * optimized bit packing for VARCHAR
   16  *
   17  **/
   21     /* packed: offset_idx (13 bits) | page_idx (19 bits) */
   38 /* 2048 values * 4 bytes = 8192 bytes = 2 * PAGE_SIZE (typically) */
   39 /* Ensuring this aligns with system page size is good for mmap */
   40 constexpr size_t CAP_PER_PAGE = 2048; 
   42 /**
   43  *
   44  * column_t: Paged vector storage
   45  * optimized for parallel construction and random access
   46  *
   47  **/
   50     /* Align to page size to prevent false sharing and help prefetchers */
   51     struct alignas(4096) Page {
   52         value_t data[CAP_PER_PAGE];
   53     };
   54 
   55     size_t num_values = 0;

include/intermediate.h --- 2/3 --- C++
78    
79      public:
80        column_t() = default;
88        /* if we know the size we can pre allocate */
89        inline void reserve(size_t expected_rows) {
90            pages.reserve((expected_rows + CAP_PER_PAGE - 1) / CAP_PER_PAGE);
93        /* appends value to page, creates new page if current page is full */
95            if (num_values % CAP_PER_PAGE == 0) {
98            pages.back()->data[num_values % CAP_PER_PAGE] = val;
102        /* pre-allocate all pages for parallel writing called before spawning threads */
103        inline void pre_allocate(size_t count) {
104            size_t pages_needed = (count + CAP_PER_PAGE - 1) / CAP_PER_PAGE;
105            pages.reserve(pages_needed);
106            for (size_t i = 0; i < pages_needed; ++i) {
107                pages.push_back(new Page());
109            num_values = count;
110        }
   67     /* Proper move semantics to avoid double-freeing pages */
   68     column_t(column_t&& other) noexcept 
   69         : num_values(other.num_values), owns_pages(other.owns_pages), 
   70           external_memory(std::move(other.external_memory)), 
   71           pages(std::move(other.pages)), 
   72           source_table(other.source_table), source_column(other.source_column) {
   73         other.owns_pages = false;
   74         other.pages.clear();
   75         other.num_values = 0;
   76     }
   78     column_t& operator=(column_t&& other) noexcept {
   79         if (this != &other) {
   80             if (owns_pages) { for (auto *p : pages) delete p; }
   82             num_values = other.num_values;
   83             owns_pages = other.owns_pages;
   84             external_memory = std::move(other.external_memory);
   85             pages = std::move(other.pages);
   86             source_table = other.source_table;
   87             source_column = other.source_column;
   89             other.owns_pages = false;
   90             other.pages.clear();
   91             other.num_values = 0;
   92         }
   93         return *this;
   94     }
   96     /* Deleted copy to prevent accidental deep copies of massive columns */
   97     column_t(const column_t&) = delete;
   98     column_t& operator=(const column_t&) = delete;
   106     /* * Resizes the column to hold `count` elements.
   107      * Allocates new pages if necessary. 
   108      * Crucial for parallel write_at access.
   109      */
   110     inline void resize(size_t count) {
   111         size_t current_capacity = pages.size() * CAP_PER_PAGE;
   112         if (count > current_capacity) {
   113             size_t needed = count - current_capacity;
   114             size_t pages_needed = (needed + CAP_PER_PAGE - 1) / CAP_PER_PAGE;
   115             pages.reserve(pages.size() + pages_needed);
   116             for (size_t i = 0; i < pages_needed; ++i) {
   117                 pages.push_back(new Page());
   118             }
   119         }
   120         num_values = count;
   123     /* * Optimizes division/modulo into bitwise operations 
   124      * because CAP_PER_PAGE (2048) is a power of 2.
   125      * 2048 = 2^11.
   126      * idx / 2048  => idx >> 11
   127      * idx % 2048  => idx & 2047
   128      */
   130         // "Unlikely" check for new page needed
   131         if ((num_values & (CAP_PER_PAGE - 1)) == 0) {
   134         pages.back()->data[num_values & (CAP_PER_PAGE - 1)] = val;
   138     /* thread-safe random write used by parallel construct_intermediate */
   139     inline void write_at(size_t idx, const value_t &val) {
   140         pages[idx >> 11]->data[idx & 0x7FF] = val;
   141     }
   143     /* read operator */
   144     const value_t &operator[](size_t idx) const {
   145         return pages[idx >> 11]->data[idx & 0x7FF];
   146     }
   148     /* non-const read operator */
   149     value_t &operator[](size_t idx) {
   150         return pages[idx >> 11]->data[idx & 0x7FF];
   153     size_t row_count() const { return num_values; }
   155     /* pre-allocate from a contiguous memory block (batch allocation) */
   156     inline void pre_allocate_from_block(void* block, size_t& offset, size_t count,
   157                                         std::shared_ptr<void> memory_keeper) {

include/intermediate.h --- 3/3 --- C++
123            owns_pages = false;
124            external_memory = memory_keeper;
125        }
126    
127        /* thread-safe random write to pre-allocated pages */
128        inline void write_at(size_t idx, const value_t &val) {
129            pages[idx / CAP_PER_PAGE]->data[idx % CAP_PER_PAGE] = val;
130        }
132        const value_t &operator[](size_t idx) const {
133            return pages[idx / CAP_PER_PAGE]->data[idx % CAP_PER_PAGE];
134        }
136        size_t row_count() const { return num_values; }
   169 };
   170 
   171 using Columnar = std::vector<column_t>;
   172 ColumnarTable to_columnar(const Columnar &table, const Plan &plan);

include/join_setup.h --- C++
157        for (size_t i = 0; i < output_attrs.size(); ++i) {
158            auto [col_idx, _] = output_attrs[i];
159    
160            mema::column_t col;
161            col.reserve(estimated_rows);
   162         if (col_idx < left_size) {
   163             set_column_metadata(col, left_input, left_node, col_idx);
   164         } else {

include/materialize.h --- 1/12 --- Text (exceeded DFT_GRAPH_LIMIT)
8    #include <plan.h>
9    #include <vector>
10    #include <worker_pool.h>
11    #include <sys/mman.h>
17     *  helper for building sparse page bitmaps
18     *  handles buffering and flushing partial bytes
29            if (is_valid) {
30                pending_bits |= (1u << bit_count);
31            }
   12 #include <string_view>
   13 #include <algorithm>
   19  * helper to centralize mmap allocation
   20  * handles error checking and throws bad_alloc
   23 inline void* allocate_pages(size_t total_pages) {
   24     void* ptr = mmap(nullptr, total_pages * PAGE_SIZE,
   25                      PROT_READ | PROT_WRITE,
   26                      MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
   27     if (ptr == MAP_FAILED) {
   28         throw std::bad_alloc();
   29     }
   30     return ptr;
   31 }
   32 
   33 /**
   34  *
   35  * helper for building sparse page bitmaps
   36  * handles buffering and flushing partial bytes
   37  * OPTIMIZED: Branchless bit setting
   38  *
   39  **/
   47     /* branchless bit setting */
   49         pending_bits |= (static_cast<uint8_t>(is_valid) << bit_count);
   50         bit_count++;
   51         if (bit_count == 8) {
   52             bitmap.push_back(pending_bits);
   53             pending_bits = 0;

include/materialize.h --- 2/12 --- Text (exceeded DFT_GRAPH_LIMIT)
52    };
53    
54    /**
55     *
56     *  extracts string view from ColumnarTable page
57     *  decodes offset array to find string boundaries
58     *  returns pointer to string data and length
59     *  works directly on page layout without copy
62    inline std::pair<const char *, uint16_t>
   74  * extracts string view from ColumnarTable page
   75  * decodes offset array to find string boundaries
   76  * returns string_view wrapper around data
   77  * works directly on page layout without copy
   80 inline std::string_view
   81 get_string_view(const Column &src_col, int32_t page_idx, int32_t offset_idx) {
   82     auto *page = reinterpret_cast<uint8_t *>(src_col.pages[page_idx]->data);
   83     auto num_valid = *reinterpret_cast<uint16_t *>(page + 2);
   84     auto *offset_array = reinterpret_cast<uint16_t *>(page + 4);

include/materialize.h --- 3/12 --- Text (exceeded DFT_GRAPH_LIMIT)
68    
69        uint16_t end_off = offset_array[offset_idx];
70        uint16_t start_off = (offset_idx == 0) ? 0 : offset_array[offset_idx - 1];
71    
72        return {char_begin + start_off, static_cast<uint16_t>(end_off - start_off)};
77     *  computes chunk size for materialization parallelization
78     *  balances cache-optimal size with enough tasks per thread
84        // calculate per-core L2 cache optimal size
89        // estimate bytes per match (conservative for VARCHAR)
93        // ensure at least 4 chunks per thread
101     *  parallel int32 materialization
102     *  each thread builds local pages then merges at end
   90     return {char_begin + start_off, static_cast<size_t>(end_off - start_off)};
   95  * computes chunk size for materialization parallelization
   96  * balances cache-optimal size with enough tasks per thread
   102     /* calculate per-core L2 cache optimal size */
   107     /* estimate bytes per match (conservative for VARCHAR) */
   111     /* ensure at least 4 chunks per thread */
   119  * parallel int32 materialization
   120  * each thread builds local pages then merges at end
   121  * OPTIMIZED: Uses "Safe Batch" calculation to remove per-row page checks
   122  *
   123  **/
   124 template <typename ReaderFunc>
   125 inline void materialize_int32_column(Column &dest_col,

include/materialize.h --- 4/12 --- Text (exceeded DFT_GRAPH_LIMIT)
109        const size_t total_matches = collector.size();
110        if (total_matches == 0) return;
111    
112        const uint64_t *matches_ptr = collector.matches.data();
113        const size_t chunk_size = compute_materialization_chunk_size(total_matches);
116        /* estimate pages per thread conservatively */
118        size_t max_rows_per_page = (PAGE_SIZE - 4 - 256) / 4;  // header + bitmap + data
122        /* batch allocate all pages upfront */
123        void* page_memory = mmap(nullptr, total_pages * PAGE_SIZE,
124                                 PROT_READ | PROT_WRITE,
125                                 MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
126        if (page_memory == MAP_FAILED) {
127            throw std::bad_alloc();
128        }
134            /* pre-assign pages to each thread's column */
   135     size_t max_rows_per_page = (PAGE_SIZE - 4 - 256) / 4;
   139     void* page_memory = allocate_pages(total_pages);
   145         for (size_t p = 0; p < pages_per_thread; ++p) {
   146             size_t page_idx = i * pages_per_thread + p;
   147             Page* page = reinterpret_cast<Page*>(
   148                 static_cast<char*>(page_memory) + page_idx * PAGE_SIZE);

include/materialize.h --- 5/12 --- Text (exceeded DFT_GRAPH_LIMIT)
142    
143        worker_pool.execute([&](size_t t, size_t num_threads) {
144            size_t start = t * total_matches / num_threads;
145            size_t end = (t + 1) * total_matches / num_threads;
146            if (start >= total_matches)
147                return;
152            /* pre-size vectors to exact capacity */
156            data.reserve(chunk_matches);  // worst case: all non-null
157            bitmap.reserve((chunk_matches + 7) / 8);  // exact bitmap size
159            /* calculate conservative min rows per page to reduce overflow checks */
160            constexpr size_t MIN_ROWS_PER_PAGE = (PAGE_SIZE - 4 - 256) / (4 + 1);  // data + bitmap
168                *reinterpret_cast<uint16_t *>(page + 2) =
169                    static_cast<uint16_t>(data.size());
171                std::memcpy(page + PAGE_SIZE - bitmap.size(), bitmap.data(),
172                            bitmap.size());
180                size_t rows_since_check = 0;
181                for (size_t i = start; i < end; ++i) {
182                    uint32_t row_id = get_row_id(matches_ptr[i]);
183                    mema::value_t val = read_value(row_id);
185                    /* check overflow only every MIN_ROWS_PER_PAGE rows */
186                    if (rows_since_check >= MIN_ROWS_PER_PAGE) {
187                        if (4 + (data.size() + 1) * 4 + (bitmap.size() + 2) > PAGE_SIZE) {
188                            save_page();
190                        rows_since_check = 0;
192    
193                    bool is_valid = !val.is_null();
194                    if (is_valid) {
195                        data.push_back(val.value);
196                    }
197                    bm_builder.add_bit(is_valid);
198                    ++num_rows;
199                    ++rows_since_check;
204                run_loop(
205                    [](uint64_t m) { return static_cast<uint32_t>(m & 0xFFFFFFFF); });
210            if (num_rows != 0)
211                save_page();
214        /* merge phase: move pages from thread-local columns to dest_col */
216            for (auto *page : thread_col.pages) {
217                dest_col.pages.push_back(page);
218            }
222        /* assign mapped memory so dest_col will munmap on destruction */
229     *  parallel varchar column materialization
230     *  each thread builds its own Column with local pages
231     *  merge pages at end via pointer moves
   156         if (start >= end) return;
   164         
   166         bitmap.reserve((chunk_matches + 7) / 8);
   168         /* 5 bytes per row conservative estimate (4 data + 1 bitmap overhead) */
   169         constexpr size_t BYTES_PER_ROW = 5;
   170         constexpr size_t HEADER_SIZE = 4; // NumRows + NumValues
   171         /* Leave some buffer for safety */
   172         constexpr size_t SAFE_PAGE_CAPACITY = PAGE_SIZE - HEADER_SIZE - 64; 
   180             *reinterpret_cast<uint16_t *>(page + 2) = static_cast<uint16_t>(data.size());
   182             std::memcpy(page + PAGE_SIZE - bitmap.size(), bitmap.data(), bitmap.size());
   183             
   191             size_t current = start;
   192             while (current < end) {
   200                     save_page();
   201                     current_bytes = 0;
   202                 }
   204                 size_t available_bytes = SAFE_PAGE_CAPACITY - current_bytes;
   209                 if (batch_size == 0) batch_size = 1;
   211                 size_t batch_end = current + batch_size;
   222                         data.push_back(val.value);
   224                     bm_builder.add_bit(is_valid);
   226                 
   227                 num_rows += static_cast<uint16_t>(batch_size);
   228                 current += batch_size;
   233             run_loop([](uint64_t m) { return static_cast<uint32_t>(m & 0xFFFFFFFF); });
   238         if (num_rows != 0) save_page();
   242         for (auto *page : thread_col.pages) dest_col.pages.push_back(page);
   252  * parallel varchar column materialization
   253  * each thread builds its own Column with local pages
   254  * OPTIMIZED: Uses reserve() instead of resize() and aggressive buffer growth
   255  *
   256  **/
   257 template <typename ReaderFunc>
   258 inline void materialize_varchar_column(Column &dest_col,

include/materialize.h --- 6/12 --- Text (exceeded DFT_GRAPH_LIMIT)
236                                           const MatchCollector &collector,
237                                           ReaderFunc &&read_value,
238                                           const Column &src_col, bool from_build) {
239        const size_t total_matches = collector.size();
240        if (total_matches == 0)
241            return;
244        const size_t chunk_size = compute_materialization_chunk_size(total_matches);
247        /* estimate pages per thread conservatively for VARCHAR */
250        size_t bytes_per_row = AVG_STRING_LEN + 2 + 1;
251        size_t usable_per_page = PAGE_SIZE - 256;
252        size_t rows_per_page = usable_per_page / bytes_per_row;
256        /* batch allocate all pages upfront */
257        void* page_memory = mmap(nullptr, total_pages * PAGE_SIZE,
258                                 PROT_READ | PROT_WRITE,
259                                 MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
260        if (page_memory == MAP_FAILED) {
261            throw std::bad_alloc();
262        }
268            /* pre-assign pages to each thread's column */
   263     if (total_matches == 0) return;
   270     size_t rows_per_page = (PAGE_SIZE - 256) / (AVG_STRING_LEN + 3);
   274     void* page_memory = allocate_pages(total_pages);
   280         for (size_t p = 0; p < pages_per_thread; ++p) {
   281             size_t page_idx = i * pages_per_thread + p;
   282             Page* page = reinterpret_cast<Page*>(
   283                 static_cast<char*>(page_memory) + page_idx * PAGE_SIZE);

include/materialize.h --- 7/12 --- Text (exceeded DFT_GRAPH_LIMIT)
276    
277        worker_pool.execute([&](size_t t, size_t num_threads) {
278            size_t start = t * total_matches / num_threads;
279            size_t end = (t + 1) * total_matches / num_threads;
280            if (start >= total_matches)
281                return;
   291         if (start >= end) return;
   292 
   293         Column &local_col = thread_columns[t];
   294         size_t chunk_matches = end - start;
   295 

include/materialize.h --- 8/12 --- Text (exceeded DFT_GRAPH_LIMIT)
288            std::vector<uint16_t> offsets;
289            std::vector<uint8_t> bitmap;
290            size_t current_char_size = 0;
291    
292            /* pre-size vectors with better estimates */
293            constexpr size_t AVG_STRING_SIZE = 32;
294            char_data.reserve(chunk_matches * AVG_STRING_SIZE);
298            /* calculate conservative min rows per page for VARCHAR */
299            constexpr size_t MIN_ROWS_PER_PAGE = 100;  // conservative for variable-length
303            auto save_long_string_buffer = [&](const std::string &str) {
308                    *reinterpret_cast<uint16_t *>(page) =
309                        first_page ? 0xffff : 0xfffe;
311                    size_t len = std::min(str.size() - offset, PAGE_SIZE - 4);
312                    *reinterpret_cast<uint16_t *>(page + 2) =
313                        static_cast<uint16_t>(len);
   302         char_data.reserve(chunk_matches * 32);
   306         /* use block checking to reduce overflow overhead */
   307         constexpr size_t CHECK_INTERVAL = 32; 
   311         auto save_long_string_buffer = [&](std::string_view str) {
   316                 *reinterpret_cast<uint16_t *>(page) = first_page ? 0xffff : 0xfffe;
   318                 size_t len = std::min(str.size() - offset, static_cast<size_t>(PAGE_SIZE - 4));
   319                 *reinterpret_cast<uint16_t *>(page + 2) = static_cast<uint16_t>(len);
   320                 std::memcpy(page + 4, str.data() + offset, len);
   321                 offset += len;
   322             }
   323         };

include/materialize.h --- 9/12 --- Text (exceeded DFT_GRAPH_LIMIT)
321                while (true) {
322                    auto *src_page_data = src_col.pages[curr_idx]->data;
323                    auto *dest_page_data = local_col.new_page()->data;
324                    std::memcpy(dest_page_data, src_page_data, PAGE_SIZE);
325    
327                    if (curr_idx >= static_cast<int32_t>(src_col.pages.size()))
328                        break;
330                    if (*reinterpret_cast<uint16_t *>(next_p) != 0xfffe)
331                        break;
335            size_t rows_since_check = 0;
336    
341                *reinterpret_cast<uint16_t *>(page + 2) =
342                    static_cast<uint16_t>(offsets.size());
344                std::memcpy(page + 4 + offsets.size() * 2, char_data.data(),
345                            current_char_size);
346                std::memcpy(page + PAGE_SIZE - bitmap.size(), bitmap.data(),
347                            bitmap.size());
353                rows_since_check = 0;
356            auto add_normal_string =
357                [&](int32_t page_idx, int32_t offset_idx) -> bool {
358                auto [str_ptr, str_len] =
359                    get_string_view(src_col, page_idx, offset_idx);
361                if (str_len > PAGE_SIZE - 7) {
362                    if (num_rows > 0)
363                        save_page();
364                    save_long_string_buffer(std::string(str_ptr, str_len));
369                                (current_char_size + str_len) +
371                if (needed > PAGE_SIZE)
372                    save_page();
374                if (current_char_size + str_len > char_data.size()) {
375                    char_data.resize(std::max(char_data.size() * 2,
376                                              current_char_size + str_len + 4096));
379                std::memcpy(char_data.data() + current_char_size, str_ptr, str_len);
380                current_char_size += str_len;
381                offsets.push_back(current_char_size);  // no emplace
391                        /* reduce overflow checks for nulls */
392                        if (rows_since_check >= MIN_ROWS_PER_PAGE) {
   332                 if (curr_idx >= static_cast<int32_t>(src_col.pages.size())) break;
   334                 if (*reinterpret_cast<uint16_t *>(next_p) != 0xfffe) break;
   342             *reinterpret_cast<uint16_t *>(page + 2) = static_cast<uint16_t>(offsets.size());
   344             std::memcpy(page + 4 + offsets.size() * 2, char_data.data(), current_char_size);
   345             std::memcpy(page + PAGE_SIZE - bitmap.size(), bitmap.data(), bitmap.size());
   346             
   351             /* Keep char_data capacity, just reset usage */
   355         auto add_normal_string = [&](int32_t page_idx, int32_t offset_idx) -> bool {
   356             auto sv = get_string_view(src_col, page_idx, offset_idx);
   358             if (sv.length() > PAGE_SIZE - 7) {
   359                 if (num_rows > 0) save_page();
   360                 save_long_string_buffer(sv);
   364             /* conservative check */
   366                             (current_char_size + sv.length()) +
   368             if (needed > PAGE_SIZE) save_page();
   370             /* aggressive growth strategy: avoid realloc */
   372                 size_t new_cap = std::max(char_data.capacity() * 2, 
   373                                           current_char_size + sv.length() + 8192);
   374                 char_data.reserve(new_cap);
   377             /* write directly to buffer without zero-init */
   379             current_char_size += sv.length();
   380             offsets.push_back(current_char_size); 
   385             size_t rows_since_check = 0;
   391                     /* check page overflow periodically */
   392                     if (++rows_since_check >= CHECK_INTERVAL) {
   393                         if (4 + offsets.size() * 2 + current_char_size +
   394                                 (bitmap.size() + 2) > PAGE_SIZE) {
   395                             save_page();
   396                         }

include/materialize.h --- 10/12 --- Text (exceeded DFT_GRAPH_LIMIT)
397                            rows_since_check = 0;
398                        }
399                        bm_builder.add_bit(false);
400                        num_rows++;
401                        rows_since_check++;
404                        mema::value_t::decode_string(val.value, page_idx,
405                                                      offset_idx);
408                            if (num_rows > 0)
409                                save_page();
416                                rows_since_check++;
424                run_loop(
425                    [](uint64_t m) { return static_cast<uint32_t>(m & 0xFFFFFFFF); });
427                run_loop(
428                    [](uint64_t m) { return static_cast<uint32_t>(m >> 32); });
431            if (num_rows != 0)
432                save_page();
435        /* merge phase: move pages from thread-local columns to dest_col */
437            for (auto *page : thread_col.pages) {
438                dest_col.pages.push_back(page);
439            }
443        /* assign mapped memory so dest_col will munmap on destruction */
450     *  materializes final ColumnarTable output from columnar inputs
451     *  both build and probe read directly from ColumnarTable pages
452     *  uses columnar_reader for efficient page access
453     *  produces final result with proper page format
456    inline ColumnarTable materialize_from_columnar(
457        const MatchCollector &collector, const JoinInput &build_input,
458        const JoinInput &probe_input,
459        const std::vector<std::tuple<size_t, DataType>> &remapped_attrs,
460        const PlanNode &build_node, const PlanNode &probe_node, size_t build_size,
461        ColumnarReader &columnar_reader, const Plan &plan) {
462    
463        ColumnarTable result;
464        result.num_rows = collector.size();
465        if (collector.size() == 0) {
466            for (size_t out_idx = 0; out_idx < remapped_attrs.size(); ++out_idx) {
467                auto [_, data_type] = remapped_attrs[out_idx];
468                result.columns.emplace_back(data_type);
469            }
470            return result;
473        auto *build_table = std::get<const ColumnarTable *>(build_input.data);
474        auto *probe_table = std::get<const ColumnarTable *>(probe_input.data);
477            auto [col_idx, data_type] = remapped_attrs[out_idx];
478            bool from_build = col_idx < build_size;
479            size_t remapped_col_idx = from_build ? col_idx : col_idx - build_size;
480    
481            const ColumnarTable *src_table = from_build ? build_table : probe_table;
482            const PlanNode *src_node = from_build ? &build_node : &probe_node;
483            auto [actual_col_idx, _] = src_node->output_attrs[remapped_col_idx];
484            const Column &src_col = src_table->columns[actual_col_idx];
485    
486            result.columns.emplace_back(data_type);
487            Column &dest_col = result.columns.back();
488    
489            auto read_columnar_value = [&](uint32_t row_id) {
490                return from_build
491                           ? columnar_reader.read_value_build(
492                                 src_col, remapped_col_idx, row_id, src_col.type)
493                           : columnar_reader.read_value_probe(
494                                 src_col, remapped_col_idx, row_id, src_col.type);
495            };
496    
497            if (data_type == DataType::INT32) {
498                materialize_int32_column(dest_col, collector, read_columnar_value,
499                                         from_build);
500            } else {
501                materialize_varchar_column(dest_col, collector, read_columnar_value,
502                                           src_col, from_build);
503            }
505    
506        return result;
511     *  materializes final ColumnarTable from mixed inputs
512     *  produces result from columnar + column_t intermediate combinations
515    inline ColumnarTable materialize_mixed(
522        ColumnarTable result;
523        result.num_rows = collector.size();
525            for (size_t out_idx = 0; out_idx < remapped_attrs.size(); ++out_idx) {
526                auto [_, data_type] = remapped_attrs[out_idx];
527                result.columns.emplace_back(data_type);
528            }
529            return result;
532        auto read_from_intermediate = [](const mema::column_t &column,
533                                         uint32_t row_id) {
534            return column[row_id];
535        };
   403                     mema::value_t::decode_string(val.value, page_idx, offset_idx);
   406                         if (num_rows > 0) save_page();
   413                             rows_since_check++; /* incremented inside add_normal too implicitly by size check */
   421             run_loop([](uint64_t m) { return static_cast<uint32_t>(m & 0xFFFFFFFF); });
   423             run_loop([](uint64_t m) { return static_cast<uint32_t>(m >> 32); });
   426         if (num_rows != 0) save_page();
   430         for (auto *page : thread_col.pages) dest_col.pages.push_back(page);
   440  * helper to determine source column pointer
   441  * handles both columnar and intermediate inputs
   444 inline std::pair<const Column*, const mema::column_t*>
   445 resolve_column_source(const JoinInput& input, const PlanNode& node, size_t col_idx) {
   446     if (input.is_columnar()) {
   447         auto* table = std::get<const ColumnarTable*>(input.data);
   448         auto [actual_idx, _] = node.output_attrs[col_idx];
   449         return { &table->columns[actual_idx], nullptr };
   450     } else {
   452         return { nullptr, &result[col_idx] };
   454 }
   456 /**
   457  *
   461  **/
   462 inline ColumnarTable create_empty_result(
   463     const std::vector<std::tuple<size_t, DataType>> &remapped_attrs) {
   464     ColumnarTable empty_result;
   465     empty_result.num_rows = 0;
   467         auto [_, data_type] = remapped_attrs[out_idx];
   468         empty_result.columns.emplace_back(data_type);
   470     return empty_result;
   475  * materializes final ColumnarTable from mixed inputs
   476  * produces result from columnar + column_t intermediate combinations
   479 inline ColumnarTable materialize(
   487         return create_empty_result(remapped_attrs);
   490     ColumnarTable result;
   491     result.num_rows = collector.size();
   492 
   493     for (size_t out_idx = 0; out_idx < remapped_attrs.size(); ++out_idx) {
   494         auto [col_idx, data_type] = remapped_attrs[out_idx];
   495         bool from_build = col_idx < build_size;

include/materialize.h --- 11/12 --- Text (exceeded DFT_GRAPH_LIMIT)
541    
542            result.columns.emplace_back(data_type);
543            Column &dest_col = result.columns.back();
544    
545            const Column *columnar_src_col = nullptr;
546            const mema::column_t *intermediate_src_col = nullptr;
548            if (from_build) {
549                if (build_input.is_columnar()) {
550                    auto *build_table =
551                        std::get<const ColumnarTable *>(build_input.data);
552                    auto [actual_col_idx, _] =
553                        build_node.output_attrs[remapped_col_idx];
554                    columnar_src_col = &build_table->columns[actual_col_idx];
555                } else {
556                    const auto &build_result =
557                        std::get<ExecuteResult>(build_input.data);
558                    intermediate_src_col = &build_result[remapped_col_idx];
559                }
560            } else {
561                if (probe_input.is_columnar()) {
562                    auto *probe_table =
563                        std::get<const ColumnarTable *>(probe_input.data);
564                    auto [actual_col_idx, _] =
565                        probe_node.output_attrs[remapped_col_idx];
566                    columnar_src_col = &probe_table->columns[actual_col_idx];
567                } else {
568                    const auto &probe_result =
569                        std::get<ExecuteResult>(probe_input.data);
570                    intermediate_src_col = &probe_result[remapped_col_idx];
571                }
572            }
573    
578                        materialize_int32_column(
579                            dest_col, collector,
580                            [&](uint32_t rid) {
581                                return columnar_reader.read_value_build(
582                                    col, remapped_col_idx, rid, DataType::INT32);
583                            },
586                        materialize_int32_column(
587                            dest_col, collector,
588                            [&](uint32_t rid) {
589                                return columnar_reader.read_value_probe(
590                                    col, remapped_col_idx, rid, DataType::INT32);
591                            },
596                    materialize_int32_column(
597                        dest_col, collector,
598                        [&](uint32_t rid) {
599                            return read_from_intermediate(col, rid);
600                        },
601                        from_build);
606                    const auto &src_table =
607                        plan.inputs[intermediate_src_col->source_table];
608                    string_source_col =
609                        &src_table.columns[intermediate_src_col->source_column];
615                        materialize_varchar_column(
616                            dest_col, collector,
617                            [&](uint32_t rid) {
618                                return columnar_reader.read_value_build(
619                                    col, remapped_col_idx, rid, DataType::VARCHAR);
620                            },
623                        materialize_varchar_column(
624                            dest_col, collector,
625                            [&](uint32_t rid) {
626                                return columnar_reader.read_value_probe(
627                                    col, remapped_col_idx, rid, DataType::VARCHAR);
628                            },
633                    materialize_varchar_column(
634                        dest_col, collector,
635                        [&](uint32_t rid) {
636                            return read_from_intermediate(col, rid);
637                        },
638                        *string_source_col, from_build);
   501         /* use helper to resolve source without duplicating logic */
   503             ? resolve_column_source(build_input, build_node, remapped_col_idx)
   504             : resolve_column_source(probe_input, probe_node, remapped_col_idx);
   510                     materialize_int32_column(dest_col, collector,
   511                         [&](uint32_t rid) { return columnar_reader.read_value_build(col, remapped_col_idx, rid, DataType::INT32); },
   514                     materialize_int32_column(dest_col, collector,
   515                         [&](uint32_t rid) { return columnar_reader.read_value_probe(col, remapped_col_idx, rid, DataType::INT32); },
   520                 materialize_int32_column(dest_col, collector,
   521                     [&](uint32_t rid) { return col[rid]; }, from_build);
   526                 const auto &src_table = plan.inputs[intermediate_src_col->source_table];
   527                 string_source_col = &src_table.columns[intermediate_src_col->source_column];
   533                     materialize_varchar_column(dest_col, collector,
   534                         [&](uint32_t rid) { return columnar_reader.read_value_build(col, remapped_col_idx, rid, DataType::VARCHAR); },
   537                     materialize_varchar_column(dest_col, collector,
   538                         [&](uint32_t rid) { return columnar_reader.read_value_probe(col, remapped_col_idx, rid, DataType::VARCHAR); },
   543                 materialize_varchar_column(dest_col, collector,
   544                     [&](uint32_t rid) { return col[rid]; }, *string_source_col, from_build);
   545             }
   546         }
   547     }
   548     return result;

include/materialize.h --- 12/12 --- Text (exceeded DFT_GRAPH_LIMIT)
643    }
644    
645    /**
646     *
647     *  materializes final ColumnarTable from column_t intermediate inputs
648     *  both build and probe are intermediate results
649     *  produces result from column_t + column_t
652    inline ColumnarTable materialize_from_intermediate(
653        const MatchCollector &collector, const ExecuteResult &build,
654        const ExecuteResult &probe,
655        const std::vector<std::tuple<size_t, DataType>> &output_attrs,
656        const Plan &plan) {
658        ColumnarTable result;
659        result.num_rows = collector.size();
660        if (collector.size() == 0) {
661            for (size_t out_idx = 0; out_idx < output_attrs.size(); ++out_idx) {
662                auto [_, data_type] = output_attrs[out_idx];
663                result.columns.emplace_back(data_type);
664            }
665            return result;
666        }
668        const uint64_t *matches_ptr = collector.matches.data();
669        const size_t total_matches = collector.size();
670        const size_t build_size = build.size();
672        for (size_t out_idx = 0; out_idx < output_attrs.size(); ++out_idx) {
673            auto [col_idx, data_type] = output_attrs[out_idx];
674            const bool from_build = col_idx < build_size;
675            const mema::column_t *column =
676                from_build ? &build[col_idx] : &probe[col_idx - build_size];
677            const uint32_t shift = from_build ? 0 : 32;
679            result.columns.emplace_back(data_type);
680            Column &dest_col = result.columns.back();
682            if (data_type == DataType::INT32) {
683                constexpr int num_threads = SPC__CORE_COUNT;
685                /* estimate pages per thread conservatively */
686                size_t matches_per_thread = (total_matches + num_threads - 1) / num_threads;
687                size_t max_rows_per_page = (PAGE_SIZE - 4 - 256) / 4;
688                size_t pages_per_thread = (matches_per_thread + max_rows_per_page - 1) / max_rows_per_page + 1;
689                size_t total_pages_int32 = pages_per_thread * num_threads;
691                /* batch allocate all pages upfront */
692                void* page_memory_int32 = mmap(nullptr, total_pages_int32 * PAGE_SIZE,
693                                         PROT_READ | PROT_WRITE,
694                                         MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
695                if (page_memory_int32 == MAP_FAILED) {
696                    throw std::bad_alloc();
697                }
699                std::vector<Column> thread_columns;
700                thread_columns.reserve(num_threads);
701                for (int i = 0; i < num_threads; ++i) {
702                    thread_columns.emplace_back(DataType::INT32);
703                    /* pre-assign pages to each thread's column */
704                    for (size_t p = 0; p < pages_per_thread; ++p) {
705                        size_t page_idx = i * pages_per_thread + p;
706                        Page* page = reinterpret_cast<Page*>(
707                            static_cast<char*>(page_memory_int32) + page_idx * PAGE_SIZE);
708                        thread_columns[i].pages.push_back(page);
709                    }
710                }
712                worker_pool.execute([&](size_t t, size_t pool_threads) {
713                    size_t start = t * total_matches / pool_threads;
714                    size_t end = (t + 1) * total_matches / pool_threads;
715                    if (start >= total_matches)
716                        return;
718                    Column &local_col = thread_columns[t];
719                    size_t chunk_matches = end - start;
721                    /* pre-size vectors to exact capacity */
722                    uint16_t num_rows = 0;
723                    std::vector<int32_t> data;
724                    std::vector<uint8_t> bitmap;
725                    data.reserve(chunk_matches);  // worst case: all non-null
726                    bitmap.reserve((chunk_matches + 7) / 8);  // exact bitmap size
728                    /* calculate conservative min rows per page to reduce overflow checks */
729                    constexpr size_t MIN_ROWS_PER_PAGE = (PAGE_SIZE - 4 - 256) / (4 + 1);
731                    BitmapBuilder bm_builder(bitmap);
733                    auto save_page = [&]() {
734                        bm_builder.flush();
735                        auto *page = local_col.new_page()->data;
736                        *reinterpret_cast<uint16_t *>(page) = num_rows;
737                        *reinterpret_cast<uint16_t *>(page + 2) =
738                            static_cast<uint16_t>(data.size());
739                        std::memcpy(page + 4, data.data(), data.size() * 4);
740                        std::memcpy(page + PAGE_SIZE - bitmap.size(), bitmap.data(),
741                                    bitmap.size());
742                        num_rows = 0;
743                        data.clear();
744                        bitmap.clear();
745                        bm_builder.reset();
746                    };
748                    size_t rows_since_check = 0;
749                    for (size_t match_idx = start; match_idx < end; ++match_idx) {
750                        uint32_t row_id = (matches_ptr[match_idx] >> shift);
751                        const mema::value_t &val = (*column)[row_id];
753                        /* check overflow only every MIN_ROWS_PER_PAGE rows */
754                        if (rows_since_check >= MIN_ROWS_PER_PAGE) {
755                            if (4 + (data.size() + 1) * 4 + (bitmap.size() + 2) > PAGE_SIZE) {
756                                save_page();
757                            }
758                            rows_since_check = 0;
759                        }
761                        bool is_valid = !val.is_null();
762                        if (is_valid) {
763                            data.push_back(val.value);
764                        }
765                        bm_builder.add_bit(is_valid);
766                        ++num_rows;
767                        ++rows_since_check;
768                    }
770                    if (num_rows != 0)
771                        save_page();
772                });
774                /* merge phase: move pages from thread-local columns to dest_col */
775                for (auto &thread_col : thread_columns) {
776                    for (auto *page : thread_col.pages) {
777                        dest_col.pages.push_back(page);
778                    }
779                    thread_col.pages.clear();
780                }
782                /* assign mapped memory so dest_col will munmap on destruction */
783                auto* mapped_mem_int32 = new MappedMemory(page_memory_int32, total_pages_int32 * PAGE_SIZE);
784                dest_col.assign_mapped_memory(mapped_mem_int32);
785            } else {
786                const auto &src_table = plan.inputs[column->source_table];
787                const auto &src_col = src_table.columns[column->source_column];
789                materialize_varchar_column(
790                    dest_col, collector,
791                    [&](uint32_t row_id) { return (*column)[row_id]; }, src_col,
792                    from_build);
793            }
794        }
795        return result;
796    }
798    /**
799     *
800     *  creates empty ColumnarTable with correct column types
801     *  used when join produces no matches at root node
802     *  initializes columns without any pages
803     *
804     **/
805    inline ColumnarTable create_empty_result(
806        const std::vector<std::tuple<size_t, DataType>> &remapped_attrs) {
807        ColumnarTable empty_result;
808        empty_result.num_rows = 0;
809        for (size_t out_idx = 0; out_idx < remapped_attrs.size(); ++out_idx) {
810            auto [_, data_type] = remapped_attrs[out_idx];
811            empty_result.columns.emplace_back(data_type);
812        }
813        return empty_result;
814    }
816    /**
817     *
818     *  dispatches materialization based on is_root flag and input types
819     *  root nodes produce ColumnarTable via materialize_* functions
820     *  intermediate nodes produce column_t via construct_intermediate_* functions
821     *  handles empty collector case for both root and intermediate
822     *
823     **/
830        bool build_is_columnar = build_input.is_columnar();
831        bool probe_is_columnar = probe_input.is_columnar();
832    
833        if (is_root) {
834            if (collector.size() == 0) {
838            if (build_is_columnar && probe_is_columnar) {
839                return materialize_from_columnar(
840                    collector, build_input, probe_input, config.remapped_attrs,
841                    build_node, probe_node, build_input.output_size(),
842                    setup.columnar_reader, plan);
843            } else if (!build_is_columnar && !probe_is_columnar) {
844                const auto &build_result =
845                    std::get<ExecuteResult>(build_input.data);
846                const auto &probe_result =
847                    std::get<ExecuteResult>(probe_input.data);
848                return materialize_from_intermediate(
849                    collector, build_result, probe_result, config.remapped_attrs,
850                    plan);
851            } else {
852                return materialize_mixed(
853                    collector, build_input, probe_input, config.remapped_attrs,
854                    build_node, probe_node, build_input.output_size(),
855                    setup.columnar_reader, plan);
856            }
858            if (collector.size() == 0) {
859                return std::move(setup.results);
860            }
861    
862            if (build_is_columnar && probe_is_columnar) {
863                construct_intermediate_from_columnar(collector, build_input, probe_input,
864                                          config.remapped_attrs, build_node,
865                                          probe_node, build_input.output_size(),
866                                          setup.columnar_reader, setup.results);
867            } else if (!build_is_columnar && !probe_is_columnar) {
868                const auto &build_result =
869                    std::get<ExecuteResult>(build_input.data);
870                const auto &probe_result =
871                    std::get<ExecuteResult>(probe_input.data);
872                construct_intermediate_from_intermediate(collector, build_result, probe_result,
873                                              config.remapped_attrs, setup.results);
874            } else {
875                construct_intermediate_mixed(collector, build_input, probe_input,
876                                  config.remapped_attrs, build_node, probe_node,
877                                  build_input.output_size(), setup.columnar_reader,
878                                  setup.results);
879            }
   553  * dispatches materialization based on is_root flag and input types
   554  * root nodes produce ColumnarTable via materialize_* functions
   556  * handles empty collector case for both root and intermediate
   565     if (collector.size() == 0) {
   566         if (is_root) {
   569         return std::move(setup.results);
   570     }
   572     if (is_root) {
   573         return std::move(materialize(
   574             collector, build_input, probe_input, config.remapped_attrs,
   575             build_node, probe_node, build_input.output_size(),
   576             setup.columnar_reader, plan));
   578         construct_intermediate(collector, build_input, probe_input,
   579                                config.remapped_attrs, build_node, probe_node,
   580                                build_input.output_size(), setup.columnar_reader,
   581                                setup.results);
   582 
   583         return std::move(setup.results);
   584     }
   585 }

